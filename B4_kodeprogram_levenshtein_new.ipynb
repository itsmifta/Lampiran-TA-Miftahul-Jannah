{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textdistance\n",
    "import datetime\n",
    "import os\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string similarity measure levenshtein\n",
    "\n",
    "def levenshtein_slice(in_f, size, start, stop):\n",
    "    out_f = \"levenshteinnew\"+str(start)+\"-\"+str(stop-1)+\".csv\"\n",
    "    \n",
    "    print(datetime.datetime.now()) # print waktu sekarang\n",
    "    df_tokenized = pd.read_csv(in_f, delimiter=',', sep=\"\\n\") # read inputan data\n",
    "    \n",
    "    # some stuff to know the progress\n",
    "    ch = 0 \n",
    "    df_dummy = pd.DataFrame({'levenshtein': [1]})\n",
    "    df_dummy.to_csv(\"levenshteinnewtxt\"+str(start)+\"-\"+str(stop-1)+\"_\"+str(ch)+\".txt\")\n",
    "    \n",
    "    df_slice = df_tokenized.iloc[start:stop , :] # men slice data (berbeda tiap kernel)\n",
    "    nameslice = \"slice \"+str(start)+\"-\"+str(stop-1)+\".csv\" # nama untuk slice data yang akan dijadikan csv\n",
    "    df_slice.to_csv(nameslice, header=['code','ingredient']) # menjadikan slice data menjadi csv\n",
    "    \n",
    "    reader = pd.read_csv(nameslice, delimiter=',', sep=\"\\n\", chunksize=size, index_col=0) # meload slice data, dengan CHUNKSIZE\n",
    "    en=0\n",
    "    \n",
    "    for chunk in reader:\n",
    "        err=''\n",
    "        il=0\n",
    "        df_lv = pd.DataFrame({'ingredient1': [],'ingredient2': [], 'levenshtein': []}) \n",
    "        for index, row in chunk.iterrows():\n",
    "            text1 = row['ingredient']\n",
    "            for index2, row2 in islice(df_tokenized.iterrows(), index+1, None):\n",
    "                text2 = row2['ingredient']\n",
    "                try:\n",
    "                    sim = textdistance.levenshtein.normalized_similarity(text1,text2)\n",
    "                    il+=1\n",
    "                    if sim>=0.8:\n",
    "                        df_lv = df_lv.append({'ingredient1': row['ingredient'],'ingredient2': row2['ingredient'], 'levenshtein': sim}, ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    err = str(e)\n",
    "                    en+=1\n",
    "                    continue\n",
    "        \n",
    "        df_lv.to_csv(out_f, index=False, header=False, mode='a')\n",
    "        renamebefore = \"levenshteinnewtxt\"+str(start)+\"-\"+str(stop-1)+\"_\"+str(ch)+\".txt\"\n",
    "        ch+=1\n",
    "        renameafter = \"levenshteinnewtxt\"+str(start)+\"-\"+str(stop-1)+\"_\"+str(ch)+\".txt\"\n",
    "        os.rename(renamebefore, renameafter)\n",
    "    print(datetime.datetime.now())\n",
    "    print(err+str(en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-29 14:15:19.588284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3191: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "levenshtein_slice(\"tokenized_nodup_tahap2.csv\",2,31064,34947)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
